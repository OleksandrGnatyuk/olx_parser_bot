import csv
import random
import time
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from logger_ import get_logger
from dotenv import load_dotenv
import os
import urllib.parse # –ü–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó URL —Ç–∞ urljoin

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
load_dotenv()

# –°–ª–æ–≤–Ω–∏–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –º—ñ—Å—è—Ü—ñ–≤ —É —Ä–æ–¥–æ–≤–æ–º—É –≤—ñ–¥–º—ñ–Ω–∫—É –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –¥–∞—Ç–∏
UKRAINIAN_MONTHS_GENITIVE = {
    1: "—Å—ñ—á–Ω—è", 2: "–ª—é—Ç–æ–≥–æ", 3: "–±–µ—Ä–µ–∑–Ω—è", 4: "–∫–≤—ñ—Ç–Ω—è", 5: "—Ç—Ä–∞–≤–Ω—è",
    6: "—á–µ—Ä–≤–Ω—è", 7: "–ª–∏–ø–Ω—è", 8: "—Å–µ—Ä–ø–Ω—è", 9: "–≤–µ—Ä–µ—Å–Ω—è", 10: "–∂–æ–≤—Ç–Ω—è",
    11: "–ª–∏—Å—Ç–æ–ø–∞–¥–∞", 12: "–≥—Ä—É–¥–Ω—è"
}

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
if not TELEGRAM_BOT_TOKEN:
    raise ValueError("TELEGRAM_BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω–∏–π –≤ .env")

TELEGRAM_CHAT_ID_OLEKSANDR = os.getenv("TELEGRAM_CHAT_ID_OLEKSANDR")
if not TELEGRAM_CHAT_ID_OLEKSANDR:
    raise ValueError("TELEGRAM_CHAT_ID_OLEKSANDR –Ω–µ –∑–∞–¥–∞–Ω–∏–π –≤ .env")

# –®–ª—è—Ö –¥–æ CSV —Ñ–∞–π–ª—É (–∫—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ os.path.join –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)
CSV_FILE_PATH = os.path.join("csv", "all_ad.csv")

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó csv, —è–∫—â–æ –≤–æ–Ω–∞ –Ω–µ —ñ—Å–Ω—É—î
os.makedirs(os.path.dirname(CSV_FILE_PATH), exist_ok=True)

logger = get_logger(__name__)

# –°—Ç–∞—Ä—Ç–æ–≤–∏–π URL
start_url = "https://www.olx.ua/uk/nedvizhimost/kvartiry/dolgosrochnaya-arenda-kvartir/kiev/?currency=UAH&search%5Bfilter_float_total_area:to%5D=60&search%5Bfilter_enum_furnish%5D%5B0%5D=yes&search%5Bfilter_enum_number_of_rooms_string%5D%5B0%5D=dvuhkomnatnye"

# –°–ø–∏—Å–æ–∫ User-Agent –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
headers_list = [
    {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36"}
]

# --- –§—É–Ω–∫—Ü—ñ—ó-—Ö–µ–ª–ø–µ—Ä–∏ ---
def send_telegram_message_oleksandr(message):
    """–í—ñ–¥–ø—Ä–∞–≤–ª—è—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ Telegram."""
    telegram_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    params = {
        "chat_id": TELEGRAM_CHAT_ID_OLEKSANDR,
        "text": message,
        "parse_mode": "HTML", # –î–æ–∑–≤–æ–ª—è—î —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è HTML —É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è—Ö
        "disable_web_page_preview": True # –ú–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏, —â–æ–± –ø—Ä–∏–±—Ä–∞—Ç–∏ –ø—Ä–µ–≤'—é –ø–æ—Å–∏–ª–∞–Ω—å
    }
    try:
        response = requests.get(telegram_url, params=params, timeout=10)
        response.raise_for_status()
        logger.info(f"–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram.")
    except requests.exceptions.RequestException as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ Telegram: {e}")


def delay():
    """–†–æ–±–∏—Ç—å –≤–∏–ø–∞–¥–∫–æ–≤—É –∑–∞—Ç—Ä–∏–º–∫—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏."""
    sleep_time = random.uniform(5, 10) # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ uniform –¥–ª—è –Ω–µ—Ü—ñ–ª–∏—Ö –∑–Ω–∞—á–µ–Ω—å
    logger.info(f"–ó–∞—Ç—Ä–∏–º–∫–∞ –Ω–∞ {sleep_time:.2f} —Å–µ–∫—É–Ω–¥...")
    time.sleep(sleep_time)


def get_html(url):
    """–û—Ç—Ä–∏–º—É—î HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º User-Agent."""
    headers = random.choice(headers_list)
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status() # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ HTTP –ø–æ–º–∏–ª–∫–∏ (4xx, 5xx)
        logger.info(f"–£—Å–ø—ñ—à–Ω–æ –æ—Ç—Ä–∏–º–∞–Ω–æ HTML –∑ {url}")
        return response.text
    except requests.exceptions.Timeout:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ Timeout –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ {url}")
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ {url}: {e}")
        return None
    except Exception as e:
        logger.error(f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ HTML –∑ {url}: {e}")
        return None

# --- –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó URL –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó ---


def generate_olx_pagination_urls(base_url: str, total_pages: int) -> list[str]:
    """
    –ì–µ–Ω–µ—Ä—É—î —Å–ø–∏—Å–æ–∫ URL-–∞–¥—Ä–µ—Å –¥–ª—è –≤—Å—ñ—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó OLX.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø—Ä–æ—Å—Ç–∏–π –º–µ—Ç–æ–¥ –¥–æ–¥–∞–≤–∞–Ω–Ω—è '&page=N'.
    """
    if total_pages < 1:
        return []

    urls = [base_url] # –ü–µ—Ä—à–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞

    if total_pages > 1:
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ, —á–∏ —î –≤–∂–µ '?' —É URL
        separator = '&' if '?' in base_url else '?'
        # –ë–∞–∑–æ–≤–∏–π URL –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (–º–æ–∂–µ –±—É—Ç–∏ —Ç–æ–π —Å–∞–º–∏–π, —â–æ –π start_url)
        url_root = base_url

        for page_num in range(2, total_pages + 1):
            page_url = f"{url_root}{separator}page={page_num}"
            # –Ø–∫—â–æ –º–∏ –¥–æ–¥–∞–ª–∏ –ø–µ—Ä—à–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä –∑ '&', –∑–∞–º—ñ–Ω—é—î–º–æ –π–æ–≥–æ –Ω–∞ '?'
            # –¶–µ–π –±–ª–æ–∫ –ø–æ—Ç—Ä—ñ–±–µ–Ω, —è–∫—â–æ –±–∞–∑–æ–≤–∏–π URL –Ω–µ –º–∞–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –≤–∑–∞–≥–∞–ª—ñ
            if separator == '?' and page_num == 2:
                separator = '&' # –ù–∞—Å—Ç—É–ø–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –±—É–¥—É—Ç—å –¥–æ–¥–∞–≤–∞—Ç–∏—Å—è —á–µ—Ä–µ–∑ '&'
            urls.append(page_url)
    return urls


def get_all_olx_urls(start_url: str) -> list[str]:
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫ —ñ –≥–µ–Ω–µ—Ä—É—î –≤—Å—ñ URL –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó.
    """
    logger.info(f"–ü–æ—à—É–∫ –∑–∞–≥–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å—Ç–æ—Ä—ñ–Ω–æ–∫, –ø–æ—á–∏–Ω–∞—é—á–∏ –∑: {start_url}")
    first_page_html = get_html(start_url)

    if not first_page_html:
        logger.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ HTML –ø–µ—Ä—à–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏. –ü–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—å–æ–≥–æ —Å–ø–∏—Å–∫—É.")
        return []

    soup = BeautifulSoup(first_page_html, "lxml")

    pagination_items = soup.select('ul[data-testid="pagination-list"] li[data-testid^="pagination-list-item"]')
    last_page_num = 1

    if pagination_items:
        try:
            last_item_li = pagination_items[-1]
            last_page_anchor = last_item_li.find('a')
            if last_page_anchor and last_page_anchor.text.strip().isdigit():
                 last_page_num = int(last_page_anchor.text.strip())
                 logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–º–µ—Ä –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {last_page_num}")
            else:
                 aria_label = last_item_li.get('aria-label', '')
                 if 'Page' in aria_label:
                     last_page_num = int(aria_label.split()[-1])
                     logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–º–µ—Ä –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ aria-label: {last_page_num}")
                 elif len(pagination_items) <= 1 and not soup.find("a", {'data-testid':"pagination-forward"}):
                      last_page_num = 1 # –Ø–∫—â–æ —Ç—ñ–ª—å–∫–∏ 1 –µ–ª–µ–º–µ–Ω—Ç —ñ –Ω–µ–º–∞—î –∫–Ω–æ–ø–∫–∏ "–≤–ø–µ—Ä–µ–¥"
                      logger.info("–ó–Ω–∞–π–¥–µ–Ω–æ –ª–∏—à–µ –æ–¥–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É.")
                 else:
                    logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è —Ç–æ—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–æ–º–µ—Ä –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –∞–±–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó.")
                    # –Ø–∫ –∑–∞–ø–∞—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç, –º–æ–∂–Ω–∞ —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É —ñ –ø–æ–¥–∏–≤–∏—Ç–∏—Å—å, —á–∏ –≤–æ–Ω–∞ —ñ—Å–Ω—É—î,
                    # –∞–ª–µ —Ü–µ —É—Å–∫–ª–∞–¥–Ω–∏—Ç—å –ª–æ–≥—ñ–∫—É. –ü–æ–∫–∏ —â–æ –∑—É–ø–∏–Ω–∏–º–æ—Å—å –Ω–∞ 1.
                    last_page_num = 1

        except (IndexError, ValueError, TypeError) as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—ñ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {e}. –ü—Ä–∏–ø—É—Å–∫–∞—î–º–æ, —â–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –æ–¥–Ω–∞.")
            last_page_num = 1
    else:
        if soup.find("div", {"data-testid": "no-search-results"}):
             logger.info("–û–≥–æ–ª–æ—à–µ–Ω—å –∑–∞ –∑–∞–ø–∏—Ç–æ–º –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
             return []
        else:
             logger.warning("–ë–ª–æ–∫ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ú–æ–∂–ª–∏–≤–æ, —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ª–∏—à–µ –æ–¥–Ω–∞.")
             last_page_num = 1

    logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è URL –¥–ª—è {last_page_num} —Å—Ç–æ—Ä—ñ–Ω–æ–∫...")
    all_urls = generate_olx_pagination_urls(start_url, last_page_num)
    logger.info(f"–£—Å–ø—ñ—à–Ω–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(all_urls)} URL.")
    return all_urls


# --- –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–∞ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö ---
def scrape_ads_from_page(page_url: str) -> list[dict]:
    """–ü–∞—Ä—Å–∏—Ç—å –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –∑ –æ–¥–Ω—ñ—î—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤."""
    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {page_url}")
    html_content = get_html(page_url)
    scraped_ads = []

    if not html_content:
        logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {page_url}")
        return scraped_ads

    soup = BeautifulSoup(html_content, "lxml")
    all_apartments_containers = soup.select('div.css-l9drzq')

    if not all_apartments_containers:
        logger.warning(
            f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ–≤ –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ: {page_url}. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä 'div.css-l9drzq'.")

    for item in all_apartments_containers:
        ad_data = {}
        try:
            # –ù–∞–∑–≤–∞
            name_tag = item.select_one("h4, h6")
            ad_data['name'] = name_tag.text.strip() if name_tag else "N/A"

            # --- –ó–º—ñ–Ω–µ–Ω–∏–π –±–ª–æ–∫ –¥–ª—è –õ–æ–∫–∞—Ü—ñ—ó —Ç–∞ –ß–∞—Å—É ---
            location_time_tag = item.select_one("p.css-vbz67q, p[data-testid='location-date']")
            time_value = "N/A"  # –ó–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
            location_value = "N/A"  # –ó–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º

            if location_time_tag:
                location_time_text = location_time_tag.text.split(" - ")
                location_value = location_time_text[0].strip() if len(location_time_text) > 0 else "N/A"

                # –û–±—Ä–æ–±–∫–∞ —á–∞—Å—É
                if len(location_time_text) > 1:
                    original_time_text = location_time_text[1].strip()
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ç–µ–∫—Å—Ç –º—ñ—Å—Ç–∏—Ç—å —Å–ª–æ–≤–æ "–°—å–æ–≥–æ–¥–Ω—ñ" (—ñ–≥–Ω–æ—Ä—É—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä)
                    if '—Å—å–æ–≥–æ–¥–Ω—ñ' in original_time_text.lower():
                        try:
                            # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—Ç–æ—á–Ω—É –¥–∞—Ç—É
                            today_date = datetime.now().date()
                            day = today_date.day
                            month_num = today_date.month
                            year = today_date.year
                            # –§–æ—Ä–º–∞—Ç—É—î–º–æ —É –ø–æ—Ç—Ä—ñ–±–Ω–∏–π —Ä—è–¥–æ–∫ –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–º –º—ñ—Å—è—Ü–µ–º
                            month_name = UKRAINIAN_MONTHS_GENITIVE.get(month_num, f"({month_num})")  # –†–µ–∑–µ—Ä–≤–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
                            time_value = f"{day:02d} {month_name} {year} —Ä."  # :02d –¥–ª—è –¥–Ω—è —Ç–∏–ø—É 01, 02...
                            logger.debug(f"–ó–∞–º—ñ–Ω–µ–Ω–æ '–°—å–æ–≥–æ–¥–Ω—ñ' –Ω–∞ '{time_value}' –¥–ª—è –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è '{ad_data['name']}'")
                        except Exception as e:
                            logger.warning(
                                f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥—Ñ–æ—Ä–º–∞—Ç—É–≤–∞—Ç–∏ '–°—å–æ–≥–æ–¥–Ω—ñ' –≤ –¥–∞—Ç—É: {e}. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç.")
                            time_value = original_time_text  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ—Å—å –¥–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
                    else:
                        # –Ø–∫—â–æ –Ω–µ "–°—å–æ–≥–æ–¥–Ω—ñ", –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç
                        time_value = original_time_text
                # –Ø–∫—â–æ –≤ split –Ω–µ –±—É–ª–æ –¥—Ä—É–≥–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞, time_value –∑–∞–ª–∏—à–∏—Ç—å—Å—è "N/A"
            # –ü—Ä–∏—Å–≤–æ—é—î–º–æ –æ—Ç—Ä–∏–º–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            ad_data['location'] = location_value
            ad_data['time'] = time_value

            # –¶—ñ–Ω–∞
            price_tag = item.select_one("p.css-uj7mm0, p[data-testid='price']")
            ad_data['price'] = price_tag.text.strip() if price_tag else "N/A"

            # –ü–ª–æ—â–∞
            square_tag = item.select_one("span.css-6as4g5")
            ad_data['square'] = square_tag.text.strip() if square_tag else ""

            # –ü–æ—Å–∏–ª–∞–Ω–Ω—è
            link_anchor = item.select_one("a")
            if link_anchor and link_anchor.get('href'):
                relative_link = link_anchor.get('href')
                ad_data['link'] = urllib.parse.urljoin(page_url, relative_link)
            else:
                ad_data['link'] = "N/A"

            if ad_data['name'] != "N/A" and ad_data['link'] != "N/A":
                scraped_ads.append(ad_data)

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –æ–∫—Ä–µ–º–æ–≥–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –Ω–∞ {page_url}: {e}", exc_info=True)

    logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(scraped_ads)} –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ.")
    return scraped_ads


def read_existing_ad_links(csv_filepath: str) -> set[str]:
    """–ß–∏—Ç–∞—î —ñ—Å–Ω—É—é—á—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –∑ CSV —Ñ–∞–π–ª—É."""
    existing_links = set()
    try:
        with open(csv_filepath, "r", encoding="utf-8", newline="") as file:
            reader = csv.reader(file)
            header = next(reader, None) # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫, —è–∫—â–æ –≤—ñ–Ω —î
            if header: # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —Ñ–∞–π–ª –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
                 # –í–∏–∑–Ω–∞—á–∞—î–º–æ —ñ–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º (–ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ, —â–æ —Ü–µ –æ—Å—Ç–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∞)
                link_index = len(header) - 1
                for row in reader:
                    if len(row) > link_index: # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —Ä—è–¥–æ–∫ –º–∞—î –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫–æ–ª–æ–Ω–æ–∫
                        existing_links.add(row[link_index])
    except FileNotFoundError:
        logger.info("CSV —Ñ–∞–π–ª —â–µ –Ω–µ —ñ—Å–Ω—É—î. –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤–∏–π.")
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è CSV —Ñ–∞–π–ª—É {csv_filepath}: {e}")
    return existing_links


def save_new_ads_to_csv(csv_filepath: str, new_ads: list[dict]):
    """–î–æ–¥–∞—î –Ω–æ–≤—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –¥–æ CSV —Ñ–∞–π–ª—É."""
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ, —á–∏ —ñ—Å–Ω—É—î —Ñ–∞–π–ª —ñ —á–∏ –≤—ñ–Ω –ø–æ—Ä–æ–∂–Ω—ñ–π, —â–æ–± –¥–æ–¥–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    file_exists = os.path.isfile(csv_filepath)
    is_empty = not file_exists or os.path.getsize(csv_filepath) == 0

    try:
        with open(csv_filepath, "a", encoding="utf-8", newline="") as file:
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ (–≤–∞–∂–ª–∏–≤–æ!)
            fieldnames = ['time', 'name', 'location', 'price', 'square', 'link']
            writer = csv.DictWriter(file, fieldnames=fieldnames)

            if is_empty:
                writer.writeheader() # –ó–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ª–∏—à–µ —è–∫—â–æ —Ñ–∞–π–ª –Ω–æ–≤–∏–π/–ø–æ—Ä–æ–∂–Ω—ñ–π

            # –ó–∞–ø–∏—Å—É—î–º–æ –¥–∞–Ω—ñ –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å
            for ad_dict in new_ads:
                 # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—å, —â–æ —Å–ª–æ–≤–Ω–∏–∫ –º—ñ—Å—Ç–∏—Ç—å –≤—Å—ñ –∫–ª—é—á—ñ –∑ fieldnames
                 row_to_write = {key: ad_dict.get(key, '') for key in fieldnames}
                 writer.writerow(row_to_write)
        logger.info(f"–£—Å–ø—ñ—à–Ω–æ –¥–æ–¥–∞–Ω–æ {len(new_ads)} –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –¥–æ {csv_filepath}")
    except IOError as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –¥–æ CSV —Ñ–∞–π–ª—É {csv_filepath}: {e}")
    except Exception as e:
        logger.error(f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å—ñ –¥–æ CSV: {e}")


# --- –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è ---

def main():
    """–û—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –ø–∞—Ä—Å–∏–Ω–≥—É."""
    logger.info("===== –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ OLX =====")

    # 1. –û—Ç—Ä–∏–º–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö URL —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
    all_page_urls = get_all_olx_urls(start_url)
    if not all_page_urls:
        logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ URL —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É. –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏.")
        send_telegram_message_oleksandr("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ URL —Å—Ç–æ—Ä—ñ–Ω–æ–∫ OLX –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É.")
        return

    logger.info(f"–ë—É–¥–µ –æ–±—Ä–æ–±–ª–µ–Ω–æ {len(all_page_urls)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫.")

    # 2. –ü—Ä–æ—á–∏—Ç–∞—Ç–∏ —ñ—Å–Ω—É—é—á—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ CSV
    existing_links = read_existing_ad_links(CSV_FILE_PATH)
    logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(existing_links)} —ñ—Å–Ω—É—é—á–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å —É CSV.")

    # 3. –°–ø–∞—Ä—Å–∏—Ç–∏ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –∑ —É—Å—ñ—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫
    all_scraped_ads = []
    for page_url in all_page_urls:
        ads_from_page = scrape_ads_from_page(page_url)
        all_scraped_ads.extend(ads_from_page)
        if page_url != all_page_urls[-1]: # –ù–µ —Ä–æ–±–∏–º–æ –∑–∞—Ç—Ä–∏–º–∫—É –ø—ñ—Å–ª—è –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
             delay()

    logger.info(f"–í—Å—å–æ–≥–æ –∑—ñ–±—Ä–∞–Ω–æ {len(all_scraped_ads)} –æ–≥–æ–ª–æ—à–µ–Ω—å –∑—ñ –≤—Å—ñ—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫.")

    # 4. –í–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–æ–≤—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
    new_ads_found = []
    processed_links_this_run = set() # –©–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –¥—É–±–ª—ñ–≤ –∑ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É

    for ad in all_scraped_ads:
        ad_link = ad.get('link')
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î –ø–æ—Å–∏–ª–∞–Ω–Ω—è, —á–∏ –≤–æ–Ω–æ –Ω–µ N/A, —á–∏ –π–æ–≥–æ –Ω–µ–º–∞—î –≤ —ñ—Å–Ω—É—é—á–∏—Ö —ñ —á–∏ –Ω–µ –æ–±—Ä–æ–±–ª—è–ª–∏ –π–æ–≥–æ –≤–∂–µ
        if ad_link and ad_link != "N/A" and ad_link not in existing_links and ad_link not in processed_links_this_run:
            new_ads_found.append(ad)
            processed_links_this_run.add(ad_link) # –î–æ–¥–∞—î–º–æ –¥–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –≤ —Ü—å–æ–º—É –∑–∞–ø—É—Å–∫—É

    logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(new_ads_found)} –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å.")

    # 5. –ó–±–µ—Ä–µ–≥—Ç–∏ –Ω–æ–≤—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è —Ç–∞ –≤—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏ –≤ Telegram
    if new_ads_found:
        save_new_ads_to_csv(CSV_FILE_PATH, new_ads_found)
        logger.info(f"–í—ñ–¥–ø—Ä–∞–≤–∫–∞ {len(new_ads_found)} –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –≤ Telegram...")
        send_telegram_message_oleksandr(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å: {len(new_ads_found)}")
        for ad in new_ads_found:
            # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ –¥–∞–Ω–∏—Ö —Å–ª–æ–≤–Ω–∏–∫–∞
            msg = (
                f"üè† <b>{ad.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∏')}</b>\n"
                f"üìç {ad.get('location', 'N/A')}\n"
                f"üí∞ {ad.get('price', 'N/A')}\n"
                f"üìè {ad.get('square', '')}\n" # –Ø–∫—â–æ square –ø–æ—Ä–æ–∂–Ω—ñ–π, –Ω—ñ—á–æ–≥–æ –Ω–µ –≤–∏–≤–µ–¥–µ
                f"‚è∞ {ad.get('time', 'N/A')}\n"
                f"üîó <a href='{ad.get('link')}'>–ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –Ω–∞ OLX</a>"
            )
            send_telegram_message_oleksandr(msg)
            time.sleep(1) # –ù–µ–≤–µ–ª–∏–∫–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è–º–∏ –≤ Telegram
        logger.info("üì® –ù–æ–≤—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ.")
    else:
        logger.info("‚ùå –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ–º–∞—î.")
        send_telegram_message_oleksandr("‚ÑπÔ∏è –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")

    logger.info("===== –ü–∞—Ä—Å–µ—Ä OLX –∑–∞–≤–µ—Ä—à–∏–≤ —Ä–æ–±–æ—Ç—É =====")


def start_parsing():
    main()


# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É ---
if __name__ == "__main__":
    main()