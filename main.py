import csv
import random
import time
import requests
from bs4 import BeautifulSoup
import logging
from dotenv import load_dotenv
import os

load_dotenv()

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
if not TELEGRAM_BOT_TOKEN:
    raise ValueError("BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env")

TELEGRAM_CHAT_ID_OLEKSANDR = os.getenv("TELEGRAM_CHAT_ID_OLEKSANDR")
if not TELEGRAM_BOT_TOKEN:
    raise ValueError("TELEGRAM_CHAT_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env")

CSV_FILE_PATH = "csv/all_ad.csv"

logging.basicConfig(level=logging.INFO)

urls = [
    "https://www.olx.pl/nieruchomosci/mieszkania/sprzedaz/lodz/?search%5Bfilter_enum_rooms%5D%5B0%5D=two&search%5Bfilter_enum_rooms%5D%5B1%5D=three&search%5Bfilter_float_m%3Afrom%5D=40&search%5Bfilter_float_price%3Afrom%5D=100000&search%5Bfilter_float_price%3Ato%5D=350000&search%5Border%5D=created_at%3Adesc",
    ]


# –°–ø–∏—Å–æ–∫ User-Agent –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –¥–ª—è —ñ–º—ñ—Ç–∞—Ü—ñ—ó —Ä–µ–∞–ª—å–Ω–∏—Ö –±—Ä–∞—É–∑–µ—Ä—ñ–≤
headers_list = [
    {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36"}
]


def send_telegram_message_oleksandr(message):
    telegram_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    params = {
        "chat_id": TELEGRAM_CHAT_ID_OLEKSANDR,
        "text": message,
        "parse_mode": "HTML",
    }
    requests.get(telegram_url, params=params)


# –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ –¥–ª—è –∑–Ω–∏–∂–µ–Ω–Ω—è –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä
def delay():
    sleep_time = random.randint(5, 10)  # –í–∏–ø–∞–¥–∫–æ–≤–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –≤—ñ–¥ 5 –¥–æ 10 —Å–µ–∫—É–Ω–¥
    time.sleep(sleep_time)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def get_html(url):
    headers = random.choice(headers_list)  # –í–∏–ø–∞–¥–∫–æ–≤–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è: {e}")
        return None


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –¥–∞–Ω–∏—Ö –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def parse_page(url):
    print(f"–ü–∞—Ä—Å–∏–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É: {url}")
    html_content = get_html(url)

    if html_content:
        soup = BeautifulSoup(html_content, "lxml")

        list_of_ap = []

        all_apartments = soup.find_all("div", class_="css-l9drzq")
        for items in all_apartments:
            name_of_ad = items.find("h4").text.strip()
            print(name_of_ad)
            local_time = items.find("p", class_="css-vbz67q").text.split(" - ")
            time = local_time[1]
            local = local_time[0]
            print(f'{time} {local}')
            price = items.find("p", class_="css-uj7mm0").text.strip()
            print(price)
            square = items.find("span", class_="css-6as4g5").text.strip()
            print(square)
            link_tag = items.find("div", class_="css-u2ayx9").find("a")
            link = f"https://olx.pl{link_tag.get('href')}" if link_tag.get("href").startswith("/") else link_tag.get(
                "href")
            print(link, end='\n')
            print('======')
            list_of_ap.append([time, name_of_ad, local, price, square, link])

        try:
            with open(CSV_FILE_PATH, "r", encoding="utf8") as file:
                reader = csv.reader(file)
                existing_names = {row[1] for row in reader}
        except FileNotFoundError:
            existing_names = set()

        new_ads = []

        with open(CSV_FILE_PATH, "a", encoding="utf8", newline="") as file:
            writer = csv.writer(file)
            for row in list_of_ap:
                if row[1] not in existing_names:
                    writer.writerow(row)
                    new_ads.append(row)

        if new_ads:
            for ad in new_ads:
                msg = f"üè† <b>{ad[1]}</b>\nüìç {ad[2]}\nüí∞ {ad[3]}\nüìè {ad[4]}\n‚è∞ {ad[0]}\nüîó <a href='{ad[5]}'>–ü–æ—Å–∏–ª–∞–Ω–Ω—è</a>"
                send_telegram_message_oleksandr(msg)
            print(f"üì® –í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {len(new_ads)} –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –≤ Telegram!")
        else:
            print("‚ùå –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ–º–∞—î.")
            send_telegram_message_oleksandr("‚ùå –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ–º–∞—î.")

    else:
        print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {url}")


async def start_parsing():
    for url in urls:
        parse_page(url)
        delay()  # –í–∏–∫–ª–∏–∫–∞—î–º–æ –∑–∞—Ç—Ä–∏–º–∫—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏