import csv
import random
import time
import requests
from bs4 import BeautifulSoup
import logging
from dotenv import load_dotenv
import os

load_dotenv()

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
if not TELEGRAM_BOT_TOKEN:
    raise ValueError("BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env")

TELEGRAM_CHAT_ID_OLEKSANDR = os.getenv("TELEGRAM_CHAT_ID_OLEKSANDR")
if not TELEGRAM_BOT_TOKEN:
    raise ValueError("TELEGRAM_CHAT_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env")

CSV_FILE_PATH = "csv/all_ad.csv"

logging.basicConfig(level=logging.INFO)

urls = [
    "https://www.olx.ua/uk/nedvizhimost/kvartiry/dolgosrochnaya-arenda-kvartir/kiev/?currency=UAH&search%5Bfilter_float_total_area:to%5D=60&search%5Bfilter_enum_furnish%5D%5B0%5D=yes&search%5Bfilter_enum_number_of_rooms_string%5D%5B0%5D=dvuhkomnatnye",
    ]


# –°–ø–∏—Å–æ–∫ User-Agent –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –¥–ª—è —ñ–º—ñ—Ç–∞—Ü—ñ—ó —Ä–µ–∞–ª—å–Ω–∏—Ö –±—Ä–∞—É–∑–µ—Ä—ñ–≤
headers_list = [
    {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"},
    {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36"}
]


def send_telegram_message_oleksandr(message):
    telegram_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    params = {
        "chat_id": TELEGRAM_CHAT_ID_OLEKSANDR,
        "text": message,
        "parse_mode": "HTML",
    }
    requests.get(telegram_url, params=params)


# –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ –¥–ª—è –∑–Ω–∏–∂–µ–Ω–Ω—è –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä
def delay():
    sleep_time = random.randint(5, 10)  # –í–∏–ø–∞–¥–∫–æ–≤–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –≤—ñ–¥ 5 –¥–æ 10 —Å–µ–∫—É–Ω–¥
    time.sleep(sleep_time)


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def get_html(url):
    headers = random.choice(headers_list)  # –í–∏–ø–∞–¥–∫–æ–≤–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è: {e}")
        return None


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –¥–∞–Ω–∏—Ö –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
def parse_page(url):
    logging.info(f"–ü–∞—Ä—Å–∏–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É: {url}")
    html_content = get_html(url)

    if html_content:
        soup = BeautifulSoup(html_content, "lxml")

        list_of_ap = []

        all_apartments = soup.select('div.css-l9drzq')

        for items in all_apartments:
            name_of_ad = items.select_one("h4").text.strip()
            local_time = items.select_one("p.css-vbz67q").text.split(" - ")
            time = local_time[1]
            local = local_time[0]
            price = items.select_one("p.css-uj7mm0").text.strip()
            square = items.select_one("span.css-6as4g5").text.strip()
            link_tag = items.select_one("div.css-u2ayx9").select_one("a")
            link = f"https://www.olx.ua{link_tag.get('href')}" if link_tag.get("href").startswith("/") else link_tag.get(
                "href")
            list_of_ap.append([time, name_of_ad, local, price, square, link])

        try:
            with open(CSV_FILE_PATH, "r", encoding="utf8") as file:
                reader = csv.reader(file)
                existing_names = {row[1] for row in reader}
        except FileNotFoundError:
            existing_names = set()

        new_ads = []

        with open(CSV_FILE_PATH, "a", encoding="utf8", newline="") as file:
            writer = csv.writer(file)
            for row in list_of_ap:
                if row[1] not in existing_names:
                    writer.writerow(row)
                    new_ads.append(row)

        if new_ads:
            for ad in new_ads:
                msg = f"üè† <b>{ad[1]}</b>\nüìç {ad[2]}\nüí∞ {ad[3]}\nüìè {ad[4]}\n‚è∞ {ad[0]}\nüîó <a href='{ad[5]}'>–ü–æ—Å–∏–ª–∞–Ω–Ω—è</a>"
                send_telegram_message_oleksandr(msg)
            logging.info(f"üì® –í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {len(new_ads)} –Ω–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –≤ Telegram!")
        else:
            logging.info("‚ùå –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ–º–∞—î.")
            send_telegram_message_oleksandr("‚ùå –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ–º–∞—î.")

    else:
        logging.info(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {url}")


async def start_parsing():
    for url in urls:
        parse_page(url)
        delay()  # –í–∏–∫–ª–∏–∫–∞—î–º–æ –∑–∞—Ç—Ä–∏–º–∫—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏

# for url in urls:
#     parse_page(url)
#     delay()